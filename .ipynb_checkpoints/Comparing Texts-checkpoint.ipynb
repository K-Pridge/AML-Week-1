{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55f254a5-8e9f-40ac-8b68-db4bee4432fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/7a/f9/85ad3071616e5abb738e229aa1fa728e26241605274a862251675ee35e27/spacy-3.7.4-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached spacy-3.7.4-cp310-cp310-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Obtaining dependency information for spacy-legacy<3.1.0,>=3.0.11 from https://files.pythonhosted.org/packages/c3/55/12e842c70ff8828e34e543a2c7176dac4da006ca6901c9e8b43efab8bc6b/spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/ed/9d/d62d12e3ecc6f99eddea6289413669a905d2ebb15cf9fe075336ca6cceaa/murmurhash-1.0.10-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached murmurhash-1.0.10-cp310-cp310-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/51/12/4aa9eec680c6d12b2275d479e159c3d063d7c757175063dd45386e15b39d/cymem-2.0.8-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached cymem-2.0.8-cp310-cp310-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/f3/72/108426ca3b6e7f16db30b3b9396e3fa45a3fd5a76f6532ab04beada2e4e3/preshed-3.0.9-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached preshed-3.0.9-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.3.0,>=8.2.2 from https://files.pythonhosted.org/packages/aa/39/7e2aa8b46b430c4d19676a4ff9f36b329788723c05e6187539daca209da2/thinc-8.2.3-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached thinc-8.2.3-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Using cached wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/0a/ed/d2c37221fe1975f4b6e8e3cf200d25b905b77e18f6a660b3dc149ade6192/srsly-2.4.8-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached srsly-2.4.8-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.4.0,>=0.1.0 from https://files.pythonhosted.org/packages/d5/e5/b63b8e255d89ba4155972990d42523251d4d1368c4906c646597f63870e2/weasel-0.3.4-py3-none-any.whl.metadata\n",
      "  Using cached weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Obtaining dependency information for typer<0.10.0,>=0.3.0 from https://files.pythonhosted.org/packages/62/39/82c9d3e10979851847361d922a373bdfef4091020da7f893acfaf07c0225/typer-0.9.4-py3-none-any.whl.metadata\n",
      "  Using cached typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from spacy) (2.29.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Obtaining dependency information for pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 from https://files.pythonhosted.org/packages/ed/76/9a17032880ed27f2dbd490c77a3431cbc80f47ba81534131de3c2846e736/pydantic-2.7.1-py3-none-any.whl.metadata\n",
      "  Using cached pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from spacy) (66.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from spacy) (23.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Obtaining dependency information for langcodes<4.0.0,>=3.2.0 from https://files.pythonhosted.org/packages/58/70/4058ab0ebb082b18d06888e711baed7f33354a5e0b363bb627586d8c323a/langcodes-3.4.0-py3-none-any.whl.metadata\n",
      "  Using cached langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from spacy) (1.23.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Obtaining dependency information for language-data>=1.2 from https://files.pythonhosted.org/packages/12/5f/139464da89c49afcc8bb97ebad48818a535220ce01b1f24c61fb80dbe4d0/language_data-1.2.0-py3-none-any.whl.metadata\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for pydantic-core==2.18.2 from https://files.pythonhosted.org/packages/c0/94/55f5b643992a57a244f7f7119b5eabebe9e592c3c8282a132ac21c965812/pydantic_core-2.18.2-cp310-none-win_amd64.whl.metadata\n",
      "  Using cached pydantic_core-2.18.2-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for typing-extensions>=4.6.1 from https://files.pythonhosted.org/packages/01/f3/936e209267d6ef7510322191003885de524fc48d1b43269810cd589ceaf5/typing_extensions-4.11.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/ad/65/d9fd07e11499e0a3162c6d61ae430172125e5c340c89c40504189d5299b9/blis-0.7.11-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached blis-0.7.11-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/39/78/f9d18da7b979a2e6007bfcea2f3c8cc02ed210538ae1ce7e69092aed7b18/confection-0.1.4-py3-none-any.whl.metadata\n",
      "  Using cached confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<0.17.0,>=0.7.0 from https://files.pythonhosted.org/packages/0f/6e/45b57a7d4573d85d0b0a39d99673dc1f5eea9d92a1a4603b35e968fbf89a/cloudpathlib-0.16.0-py3-none-any.whl.metadata\n",
      "  Using cached cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wkyle\\anaconda3\\envs\\dojo-env\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Obtaining dependency information for marisa-trie>=0.7.7 from https://files.pythonhosted.org/packages/4d/c4/638de489d6864de70167b814bf61f987af9ade696954b5cfb48aac5657c6/marisa_trie-1.1.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached marisa_trie-1.1.0-cp310-cp310-win_amd64.whl.metadata (8.8 kB)\n",
      "Using cached spacy-3.7.4-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Using cached langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Using cached preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "Using cached pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "Using cached pydantic_core-2.18.2-cp310-none-win_amd64.whl (1.9 MB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "Using cached thinc-8.2.3-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Using cached typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.6 MB 9.6 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.6/6.6 MB 8.0 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.0/6.6 MB 7.9 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.4/6.6 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.7/6.6 MB 7.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.1/6.6 MB 8.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.6/6.6 MB 8.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.9/6.6 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.1/6.6 MB 7.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.5/6.6 MB 7.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.3/6.6 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.2/6.6 MB 8.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.6/6.6 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.0/6.6 MB 8.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.4/6.6 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 8.0 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB ? eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/5.4 MB 8.6 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.7/5.4 MB 8.9 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.1/5.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.5/5.4 MB 8.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.0/5.4 MB 8.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 9.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.8/5.4 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 9.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 9.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 9.0 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Downloading marisa_trie-1.1.0-cp310-cp310-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.4/152.4 kB 9.5 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, typing-extensions, spacy-loggers, spacy-legacy, murmurhash, marisa-trie, catalogue, blis, annotated-types, typer, srsly, pydantic-core, preshed, language-data, cloudpathlib, pydantic, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.1.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.7.1 pydantic-core-2.18.2 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 typing-extensions-4.11.0 wasabi-1.1.2 weasel-0.3.4\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "from spacy.cli import download\n",
    "download('en_core_web_sm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6391f37-cc04-438c-bdaf-2dc3d125b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary packages\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59e5b0f9-b29f-4ca5-a870-008525893c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A whirlwind day in D.C. showcases Trump’s unor...</td>\n",
       "      <td>Donald Trump endorsed an unabashedly noninterv...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In Baltimore's call for federal police probe, ...</td>\n",
       "      <td>While some Justice Department investigations a...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Trump Proudly Declares: Most Of The People I’v...</td>\n",
       "      <td>Trump Proudly Declares: Most Of The People I’v...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Inside the Trump-Bush melodrama: Decades of te...</td>\n",
       "      <td>Donald Trump spent a day in January 2014 hobno...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Shutdown clash to return in force by December</td>\n",
       "      <td>Notable names include Ray Washburne (Commerce)...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  A whirlwind day in D.C. showcases Trump’s unor...   \n",
       "1           1  In Baltimore's call for federal police probe, ...   \n",
       "2           2  Trump Proudly Declares: Most Of The People I’v...   \n",
       "3           3  Inside the Trump-Bush melodrama: Decades of te...   \n",
       "4           4      Shutdown clash to return in force by December   \n",
       "\n",
       "                                                text label  \n",
       "0  Donald Trump endorsed an unabashedly noninterv...  REAL  \n",
       "1  While some Justice Department investigations a...  REAL  \n",
       "2  Trump Proudly Declares: Most Of The People I’v...  FAKE  \n",
       "3  Donald Trump spent a day in January 2014 hobno...  REAL  \n",
       "4  Notable names include Ray Washburne (Commerce)...  REAL  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/Fake_Real_News_Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e99b026-bcc1-4f4a-8dc7-e8c65c2102c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6335 entries, 0 to 6334\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  6335 non-null   int64 \n",
      " 1   title       6335 non-null   object\n",
      " 2   text        6335 non-null   object\n",
      " 3   label       6335 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 198.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f02c98ea-0caa-4859-ad99-ba79acdf389e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4053a1f-b09f-4e24-b261-2cabe27a8137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53d19ae5-23e9-4ff0-88b9-66bf1121d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6335 entries, 0 to 6334\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   6335 non-null   object\n",
      " 1   text    6335 non-null   object\n",
      " 2   label   6335 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 148.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f73ecc24-345a-491d-b00e-cd4f51a48248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A whirlwind day in D.C. showcases Trump’s unor...</td>\n",
       "      <td>Donald Trump endorsed an unabashedly noninterv...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Baltimore's call for federal police probe, ...</td>\n",
       "      <td>While some Justice Department investigations a...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump Proudly Declares: Most Of The People I’v...</td>\n",
       "      <td>Trump Proudly Declares: Most Of The People I’v...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inside the Trump-Bush melodrama: Decades of te...</td>\n",
       "      <td>Donald Trump spent a day in January 2014 hobno...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shutdown clash to return in force by December</td>\n",
       "      <td>Notable names include Ray Washburne (Commerce)...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  A whirlwind day in D.C. showcases Trump’s unor...   \n",
       "1  In Baltimore's call for federal police probe, ...   \n",
       "2  Trump Proudly Declares: Most Of The People I’v...   \n",
       "3  Inside the Trump-Bush melodrama: Decades of te...   \n",
       "4      Shutdown clash to return in force by December   \n",
       "\n",
       "                                                text label  \n",
       "0  Donald Trump endorsed an unabashedly noninterv...  REAL  \n",
       "1  While some Justice Department investigations a...  REAL  \n",
       "2  Trump Proudly Declares: Most Of The People I’v...  FAKE  \n",
       "3  Donald Trump spent a day in January 2014 hobno...  REAL  \n",
       "4  Notable names include Ray Washburne (Commerce)...  REAL  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bac368b-ab12-4254-a7f4-199682bb2c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>lower_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A whirlwind day in D.C. showcases Trump’s unor...</td>\n",
       "      <td>Donald Trump endorsed an unabashedly noninterv...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>donald trump endorsed an unabashedly noninterv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Baltimore's call for federal police probe, ...</td>\n",
       "      <td>While some Justice Department investigations a...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>while some justice department investigations a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump Proudly Declares: Most Of The People I’v...</td>\n",
       "      <td>Trump Proudly Declares: Most Of The People I’v...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>trump proudly declares: most of the people i’v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inside the Trump-Bush melodrama: Decades of te...</td>\n",
       "      <td>Donald Trump spent a day in January 2014 hobno...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>donald trump spent a day in january 2014 hobno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shutdown clash to return in force by December</td>\n",
       "      <td>Notable names include Ray Washburne (Commerce)...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>notable names include ray washburne (commerce)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  A whirlwind day in D.C. showcases Trump’s unor...   \n",
       "1  In Baltimore's call for federal police probe, ...   \n",
       "2  Trump Proudly Declares: Most Of The People I’v...   \n",
       "3  Inside the Trump-Bush melodrama: Decades of te...   \n",
       "4      Shutdown clash to return in force by December   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Donald Trump endorsed an unabashedly noninterv...  REAL   \n",
       "1  While some Justice Department investigations a...  REAL   \n",
       "2  Trump Proudly Declares: Most Of The People I’v...  FAKE   \n",
       "3  Donald Trump spent a day in January 2014 hobno...  REAL   \n",
       "4  Notable names include Ray Washburne (Commerce)...  REAL   \n",
       "\n",
       "                                          lower_text  \n",
       "0  donald trump endorsed an unabashedly noninterv...  \n",
       "1  while some justice department investigations a...  \n",
       "2  trump proudly declares: most of the people i’v...  \n",
       "3  donald trump spent a day in january 2014 hobno...  \n",
       "4  notable names include ray washburne (commerce)...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lower_text'] = df['text'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba252728-4094-45af-b3e5-838ca14d3337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>lower_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A whirlwind day in D.C. showcases Trump’s unor...</td>\n",
       "      <td>Donald Trump endorsed an unabashedly noninterv...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>donald trump endorsed an unabashedly noninterv...</td>\n",
       "      <td>[donald, trump, endorsed, an, unabashedly, non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Baltimore's call for federal police probe, ...</td>\n",
       "      <td>While some Justice Department investigations a...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>while some justice department investigations a...</td>\n",
       "      <td>[while, some, justice, department, investigati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump Proudly Declares: Most Of The People I’v...</td>\n",
       "      <td>Trump Proudly Declares: Most Of The People I’v...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>trump proudly declares: most of the people i’v...</td>\n",
       "      <td>[trump, proudly, declares:, most, of, the, peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inside the Trump-Bush melodrama: Decades of te...</td>\n",
       "      <td>Donald Trump spent a day in January 2014 hobno...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>donald trump spent a day in january 2014 hobno...</td>\n",
       "      <td>[donald, trump, spent, a, day, in, january, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shutdown clash to return in force by December</td>\n",
       "      <td>Notable names include Ray Washburne (Commerce)...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>notable names include ray washburne (commerce)...</td>\n",
       "      <td>[notable, names, include, ray, washburne, (com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  A whirlwind day in D.C. showcases Trump’s unor...   \n",
       "1  In Baltimore's call for federal police probe, ...   \n",
       "2  Trump Proudly Declares: Most Of The People I’v...   \n",
       "3  Inside the Trump-Bush melodrama: Decades of te...   \n",
       "4      Shutdown clash to return in force by December   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Donald Trump endorsed an unabashedly noninterv...  REAL   \n",
       "1  While some Justice Department investigations a...  REAL   \n",
       "2  Trump Proudly Declares: Most Of The People I’v...  FAKE   \n",
       "3  Donald Trump spent a day in January 2014 hobno...  REAL   \n",
       "4  Notable names include Ray Washburne (Commerce)...  REAL   \n",
       "\n",
       "                                          lower_text  \\\n",
       "0  donald trump endorsed an unabashedly noninterv...   \n",
       "1  while some justice department investigations a...   \n",
       "2  trump proudly declares: most of the people i’v...   \n",
       "3  donald trump spent a day in january 2014 hobno...   \n",
       "4  notable names include ray washburne (commerce)...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [donald, trump, endorsed, an, unabashedly, non...  \n",
       "1  [while, some, justice, department, investigati...  \n",
       "2  [trump, proudly, declares:, most, of, the, peo...  \n",
       "3  [donald, trump, spent, a, day, in, january, 20...  \n",
       "4  [notable, names, include, ray, washburne, (com...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'] = df['lower_text'].str.split()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d518a-3d1b-4d80-b208-5cda35008504",
   "metadata": {},
   "source": [
    "# SpaCY Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6985f2e1-fe46-4622-8959-480003f07e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f536577-47e7-4d11-9560-7f3456f341c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the model.  Disable Named Entity Recognizer (too slow)\n",
    "nlp_model = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "nlp_model.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2971e75-2eca-4756-8798-ea367f2f01ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "«Current Concerns», n°23, October 22th, 2016\n",
       "Can the great nuclear war be prevented ? Can the great war be prevented … Russia and China are preparing for war – right in front of America’s doorstep, by Niki Vogt / Alert Memorandum for Obama warned to defuse tensions with Russia, by Veteran Intelligence Professionals for Sanity VIPS / US-Mayors warn against increasing danger of war / “We are beaten to war”, Interview with Willy Wimmer / “Let us say with conviction: No to war!” / Popular initiative for nuclear phase-out in Switzerland, by Ernst Pauli / A nuclear power plant in Bolivia using lithium instead of uranium? / Prima i nostri! Ticino population tackles ruling of immigration themselves, by Marianne Wüthrich / “Defending the identity of France means saving our dairy farmers”, by Natacha Polony / The absurdity of today’s credit system, by Myret Zaki / In Great Britain, things are moving after the Brexit, by Karl Müller / Language teaching: Avoiding unnecessary quarrels, by Pierre-Gabriel Bieri.\n",
       "Partners | Zurich (Switzerland) | 27 October 2016 français Source\n",
       "Current Concerns (Switzerland)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp_model(df['text'][5])\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f11702a-b8f0-4d1c-90b5-21929f693836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[«,\n",
       " Current,\n",
       " Concerns,\n",
       " »,\n",
       " ,,\n",
       " n,\n",
       " °,\n",
       " 23,\n",
       " ,,\n",
       " October,\n",
       " 22th,\n",
       " ,,\n",
       " 2016,\n",
       " ,\n",
       " Can,\n",
       " the,\n",
       " great,\n",
       " nuclear,\n",
       " war,\n",
       " be,\n",
       " prevented,\n",
       " ?,\n",
       " Can,\n",
       " the,\n",
       " great,\n",
       " war,\n",
       " be,\n",
       " prevented,\n",
       " …,\n",
       " Russia,\n",
       " and,\n",
       " China,\n",
       " are,\n",
       " preparing,\n",
       " for,\n",
       " war,\n",
       " –,\n",
       " right,\n",
       " in,\n",
       " front,\n",
       " of,\n",
       " America,\n",
       " ’s,\n",
       " doorstep,\n",
       " ,,\n",
       " by,\n",
       " Niki,\n",
       " Vogt,\n",
       " /,\n",
       " Alert,\n",
       " Memorandum,\n",
       " for,\n",
       " Obama,\n",
       " warned,\n",
       " to,\n",
       " defuse,\n",
       " tensions,\n",
       " with,\n",
       " Russia,\n",
       " ,,\n",
       " by,\n",
       " Veteran,\n",
       " Intelligence,\n",
       " Professionals,\n",
       " for,\n",
       " Sanity,\n",
       " VIPS,\n",
       " /,\n",
       " US,\n",
       " -,\n",
       " Mayors,\n",
       " warn,\n",
       " against,\n",
       " increasing,\n",
       " danger,\n",
       " of,\n",
       " war,\n",
       " /,\n",
       " “,\n",
       " We,\n",
       " are,\n",
       " beaten,\n",
       " to,\n",
       " war,\n",
       " ”,\n",
       " ,,\n",
       " Interview,\n",
       " with,\n",
       " Willy,\n",
       " Wimmer,\n",
       " /,\n",
       " “,\n",
       " Let,\n",
       " us,\n",
       " say,\n",
       " with,\n",
       " conviction,\n",
       " :,\n",
       " No,\n",
       " to,\n",
       " war,\n",
       " !,\n",
       " ”,\n",
       " /,\n",
       " Popular,\n",
       " initiative,\n",
       " for,\n",
       " nuclear,\n",
       " phase,\n",
       " -,\n",
       " out,\n",
       " in,\n",
       " Switzerland,\n",
       " ,,\n",
       " by,\n",
       " Ernst,\n",
       " Pauli,\n",
       " /,\n",
       " A,\n",
       " nuclear,\n",
       " power,\n",
       " plant,\n",
       " in,\n",
       " Bolivia,\n",
       " using,\n",
       " lithium,\n",
       " instead,\n",
       " of,\n",
       " uranium,\n",
       " ?,\n",
       " /,\n",
       " Prima,\n",
       " i,\n",
       " nostri,\n",
       " !,\n",
       " Ticino,\n",
       " population,\n",
       " tackles,\n",
       " ruling,\n",
       " of,\n",
       " immigration,\n",
       " themselves,\n",
       " ,,\n",
       " by,\n",
       " Marianne,\n",
       " Wüthrich,\n",
       " /,\n",
       " “,\n",
       " Defending,\n",
       " the,\n",
       " identity,\n",
       " of,\n",
       " France,\n",
       " means,\n",
       " saving,\n",
       " our,\n",
       " dairy,\n",
       " farmers,\n",
       " ”,\n",
       " ,,\n",
       " by,\n",
       " Natacha,\n",
       " Polony,\n",
       " /,\n",
       " The,\n",
       " absurdity,\n",
       " of,\n",
       " today,\n",
       " ’s,\n",
       " credit,\n",
       " system,\n",
       " ,,\n",
       " by,\n",
       " Myret,\n",
       " Zaki,\n",
       " /,\n",
       " In,\n",
       " Great,\n",
       " Britain,\n",
       " ,,\n",
       " things,\n",
       " are,\n",
       " moving,\n",
       " after,\n",
       " the,\n",
       " Brexit,\n",
       " ,,\n",
       " by,\n",
       " Karl,\n",
       " Müller,\n",
       " /,\n",
       " Language,\n",
       " teaching,\n",
       " :,\n",
       " Avoiding,\n",
       " unnecessary,\n",
       " quarrels,\n",
       " ,,\n",
       " by,\n",
       " Pierre,\n",
       " -,\n",
       " Gabriel,\n",
       " Bieri,\n",
       " .,\n",
       " ,\n",
       " Partners,\n",
       " |,\n",
       " Zurich,\n",
       " (,\n",
       " Switzerland,\n",
       " ),\n",
       " |,\n",
       " 27,\n",
       " October,\n",
       " 2016,\n",
       " français,\n",
       " Source,\n",
       " ,\n",
       " Current,\n",
       " Concerns,\n",
       " (,\n",
       " Switzerland,\n",
       " )]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Display the tokens in the document\n",
    "[token for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3603e5bd-fc98-4788-b8bd-9f4a832fc343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Isolate the last token in the document\n",
    "word = doc[-1]\n",
    "\n",
    "## Display the text and type of the token\n",
    "print(word)\n",
    "type(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f4b8c17-b8e4-496d-9d33-63a507bc920b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "')'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Display the lemmatized form of the token\n",
    "word.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfae8628-3a37-4354-8d8b-27d1d4fbe456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check whether the token is punctuation\n",
    "word.is_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76d423f6-aea3-42f8-8914-a9a7c286e449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check whether the token is a space\n",
    "word.is_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4a24d01-a3de-4507-ade3-3652b1bfc884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PUNCT'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the part of speech of the token\n",
    "word.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99bc31e3-7033-4fc7-b99d-1f63246996a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PUNCT',\n",
       " 'ADJ',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'CCONJ',\n",
       " 'NUM',\n",
       " 'NUM',\n",
       " 'PUNCT',\n",
       " 'PROPN',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'NUM',\n",
       " 'SPACE',\n",
       " 'AUX',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'PUNCT',\n",
       " 'AUX',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'PUNCT',\n",
       " 'PROPN',\n",
       " 'CCONJ',\n",
       " 'PROPN',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'ADV',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PART',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'SYM',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'VERB',\n",
       " 'PART',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'SYM',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'PROPN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'PUNCT',\n",
       " 'PRON',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'PUNCT',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'PUNCT',\n",
       " 'VERB',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'INTJ',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'PUNCT',\n",
       " 'SYM',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'ADV',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'SYM',\n",
       " 'PROPN',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'PUNCT',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PRON',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'PUNCT',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'PRON',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PART',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'SYM',\n",
       " 'PROPN',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'VERB',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'SPACE',\n",
       " 'PROPN',\n",
       " 'NOUN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PROPN',\n",
       " 'NUM',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'SPACE',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'PROPN',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.pos_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb8c1dfd-0f14-4b3a-8dc2-4707e837b8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['«',\n",
       " 'current',\n",
       " 'Concerns',\n",
       " '»',\n",
       " ',',\n",
       " 'n',\n",
       " '°',\n",
       " '23',\n",
       " ',',\n",
       " 'October',\n",
       " '22th',\n",
       " ',',\n",
       " '2016',\n",
       " '\\n',\n",
       " 'can',\n",
       " 'the',\n",
       " 'great',\n",
       " 'nuclear',\n",
       " 'war',\n",
       " 'be',\n",
       " 'prevent',\n",
       " '?',\n",
       " 'can',\n",
       " 'the',\n",
       " 'great',\n",
       " 'war',\n",
       " 'be',\n",
       " 'prevent',\n",
       " '…',\n",
       " 'Russia',\n",
       " 'and',\n",
       " 'China',\n",
       " 'be',\n",
       " 'prepare',\n",
       " 'for',\n",
       " 'war',\n",
       " '–',\n",
       " 'right',\n",
       " 'in',\n",
       " 'front',\n",
       " 'of',\n",
       " 'America',\n",
       " '’s',\n",
       " 'doorstep',\n",
       " ',',\n",
       " 'by',\n",
       " 'Niki',\n",
       " 'Vogt',\n",
       " '/',\n",
       " 'Alert',\n",
       " 'Memorandum',\n",
       " 'for',\n",
       " 'Obama',\n",
       " 'warn',\n",
       " 'to',\n",
       " 'defuse',\n",
       " 'tension',\n",
       " 'with',\n",
       " 'Russia',\n",
       " ',',\n",
       " 'by',\n",
       " 'Veteran',\n",
       " 'Intelligence',\n",
       " 'Professionals',\n",
       " 'for',\n",
       " 'Sanity',\n",
       " 'VIPS',\n",
       " '/',\n",
       " 'US',\n",
       " '-',\n",
       " 'Mayors',\n",
       " 'warn',\n",
       " 'against',\n",
       " 'increase',\n",
       " 'danger',\n",
       " 'of',\n",
       " 'war',\n",
       " '/',\n",
       " '\"',\n",
       " 'we',\n",
       " 'be',\n",
       " 'beat',\n",
       " 'to',\n",
       " 'war',\n",
       " '\"',\n",
       " ',',\n",
       " 'interview',\n",
       " 'with',\n",
       " 'Willy',\n",
       " 'Wimmer',\n",
       " '/',\n",
       " '\"',\n",
       " 'let',\n",
       " 'we',\n",
       " 'say',\n",
       " 'with',\n",
       " 'conviction',\n",
       " ':',\n",
       " 'no',\n",
       " 'to',\n",
       " 'war',\n",
       " '!',\n",
       " '\"',\n",
       " '/',\n",
       " 'popular',\n",
       " 'initiative',\n",
       " 'for',\n",
       " 'nuclear',\n",
       " 'phase',\n",
       " '-',\n",
       " 'out',\n",
       " 'in',\n",
       " 'Switzerland',\n",
       " ',',\n",
       " 'by',\n",
       " 'Ernst',\n",
       " 'Pauli',\n",
       " '/',\n",
       " 'a',\n",
       " 'nuclear',\n",
       " 'power',\n",
       " 'plant',\n",
       " 'in',\n",
       " 'Bolivia',\n",
       " 'use',\n",
       " 'lithium',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'uranium',\n",
       " '?',\n",
       " '/',\n",
       " 'Prima',\n",
       " 'I',\n",
       " 'nostri',\n",
       " '!',\n",
       " 'ticino',\n",
       " 'population',\n",
       " 'tackle',\n",
       " 'ruling',\n",
       " 'of',\n",
       " 'immigration',\n",
       " 'themselves',\n",
       " ',',\n",
       " 'by',\n",
       " 'Marianne',\n",
       " 'Wüthrich',\n",
       " '/',\n",
       " '\"',\n",
       " 'defend',\n",
       " 'the',\n",
       " 'identity',\n",
       " 'of',\n",
       " 'France',\n",
       " 'mean',\n",
       " 'save',\n",
       " 'our',\n",
       " 'dairy',\n",
       " 'farmer',\n",
       " '\"',\n",
       " ',',\n",
       " 'by',\n",
       " 'Natacha',\n",
       " 'Polony',\n",
       " '/',\n",
       " 'the',\n",
       " 'absurdity',\n",
       " 'of',\n",
       " 'today',\n",
       " '’s',\n",
       " 'credit',\n",
       " 'system',\n",
       " ',',\n",
       " 'by',\n",
       " 'Myret',\n",
       " 'Zaki',\n",
       " '/',\n",
       " 'in',\n",
       " 'Great',\n",
       " 'Britain',\n",
       " ',',\n",
       " 'thing',\n",
       " 'be',\n",
       " 'move',\n",
       " 'after',\n",
       " 'the',\n",
       " 'Brexit',\n",
       " ',',\n",
       " 'by',\n",
       " 'Karl',\n",
       " 'Müller',\n",
       " '/',\n",
       " 'Language',\n",
       " 'teaching',\n",
       " ':',\n",
       " 'avoid',\n",
       " 'unnecessary',\n",
       " 'quarrel',\n",
       " ',',\n",
       " 'by',\n",
       " 'Pierre',\n",
       " '-',\n",
       " 'Gabriel',\n",
       " 'Bieri',\n",
       " '.',\n",
       " '\\n',\n",
       " 'Partners',\n",
       " '|',\n",
       " 'Zurich',\n",
       " '(',\n",
       " 'Switzerland',\n",
       " ')',\n",
       " '|',\n",
       " '27',\n",
       " 'October',\n",
       " '2016',\n",
       " 'français',\n",
       " 'Source',\n",
       " '\\n',\n",
       " 'Current',\n",
       " 'Concerns',\n",
       " '(',\n",
       " 'Switzerland',\n",
       " ')']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## [v1: List Comp] Make a list of the lemmas for each token in the document\n",
    "[token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5497682-0798-4c35-a2b5-f86b92f571ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['«',\n",
       " 'current',\n",
       " 'Concerns',\n",
       " '»',\n",
       " ',',\n",
       " 'n',\n",
       " '°',\n",
       " '23',\n",
       " ',',\n",
       " 'October',\n",
       " '22th',\n",
       " ',',\n",
       " '2016',\n",
       " '\\n',\n",
       " 'can',\n",
       " 'the',\n",
       " 'great',\n",
       " 'nuclear',\n",
       " 'war',\n",
       " 'be',\n",
       " 'prevent',\n",
       " '?',\n",
       " 'can',\n",
       " 'the',\n",
       " 'great',\n",
       " 'war',\n",
       " 'be',\n",
       " 'prevent',\n",
       " '…',\n",
       " 'Russia',\n",
       " 'and',\n",
       " 'China',\n",
       " 'be',\n",
       " 'prepare',\n",
       " 'for',\n",
       " 'war',\n",
       " '–',\n",
       " 'right',\n",
       " 'in',\n",
       " 'front',\n",
       " 'of',\n",
       " 'America',\n",
       " '’s',\n",
       " 'doorstep',\n",
       " ',',\n",
       " 'by',\n",
       " 'Niki',\n",
       " 'Vogt',\n",
       " '/',\n",
       " 'Alert',\n",
       " 'Memorandum',\n",
       " 'for',\n",
       " 'Obama',\n",
       " 'warn',\n",
       " 'to',\n",
       " 'defuse',\n",
       " 'tension',\n",
       " 'with',\n",
       " 'Russia',\n",
       " ',',\n",
       " 'by',\n",
       " 'Veteran',\n",
       " 'Intelligence',\n",
       " 'Professionals',\n",
       " 'for',\n",
       " 'Sanity',\n",
       " 'VIPS',\n",
       " '/',\n",
       " 'US',\n",
       " '-',\n",
       " 'Mayors',\n",
       " 'warn',\n",
       " 'against',\n",
       " 'increase',\n",
       " 'danger',\n",
       " 'of',\n",
       " 'war',\n",
       " '/',\n",
       " '\"',\n",
       " 'we',\n",
       " 'be',\n",
       " 'beat',\n",
       " 'to',\n",
       " 'war',\n",
       " '\"',\n",
       " ',',\n",
       " 'interview',\n",
       " 'with',\n",
       " 'Willy',\n",
       " 'Wimmer',\n",
       " '/',\n",
       " '\"',\n",
       " 'let',\n",
       " 'we',\n",
       " 'say',\n",
       " 'with',\n",
       " 'conviction',\n",
       " ':',\n",
       " 'no',\n",
       " 'to',\n",
       " 'war',\n",
       " '!',\n",
       " '\"',\n",
       " '/',\n",
       " 'popular',\n",
       " 'initiative',\n",
       " 'for',\n",
       " 'nuclear',\n",
       " 'phase',\n",
       " '-',\n",
       " 'out',\n",
       " 'in',\n",
       " 'Switzerland',\n",
       " ',',\n",
       " 'by',\n",
       " 'Ernst',\n",
       " 'Pauli',\n",
       " '/',\n",
       " 'a',\n",
       " 'nuclear',\n",
       " 'power',\n",
       " 'plant',\n",
       " 'in',\n",
       " 'Bolivia',\n",
       " 'use',\n",
       " 'lithium',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'uranium',\n",
       " '?',\n",
       " '/',\n",
       " 'Prima',\n",
       " 'I',\n",
       " 'nostri',\n",
       " '!',\n",
       " 'ticino',\n",
       " 'population',\n",
       " 'tackle',\n",
       " 'ruling',\n",
       " 'of',\n",
       " 'immigration',\n",
       " 'themselves',\n",
       " ',',\n",
       " 'by',\n",
       " 'Marianne',\n",
       " 'Wüthrich',\n",
       " '/',\n",
       " '\"',\n",
       " 'defend',\n",
       " 'the',\n",
       " 'identity',\n",
       " 'of',\n",
       " 'France',\n",
       " 'mean',\n",
       " 'save',\n",
       " 'our',\n",
       " 'dairy',\n",
       " 'farmer',\n",
       " '\"',\n",
       " ',',\n",
       " 'by',\n",
       " 'Natacha',\n",
       " 'Polony',\n",
       " '/',\n",
       " 'the',\n",
       " 'absurdity',\n",
       " 'of',\n",
       " 'today',\n",
       " '’s',\n",
       " 'credit',\n",
       " 'system',\n",
       " ',',\n",
       " 'by',\n",
       " 'Myret',\n",
       " 'Zaki',\n",
       " '/',\n",
       " 'in',\n",
       " 'Great',\n",
       " 'Britain',\n",
       " ',',\n",
       " 'thing',\n",
       " 'be',\n",
       " 'move',\n",
       " 'after',\n",
       " 'the',\n",
       " 'Brexit',\n",
       " ',',\n",
       " 'by',\n",
       " 'Karl',\n",
       " 'Müller',\n",
       " '/',\n",
       " 'Language',\n",
       " 'teaching',\n",
       " ':',\n",
       " 'avoid',\n",
       " 'unnecessary',\n",
       " 'quarrel',\n",
       " ',',\n",
       " 'by',\n",
       " 'Pierre',\n",
       " '-',\n",
       " 'Gabriel',\n",
       " 'Bieri',\n",
       " '.',\n",
       " '\\n',\n",
       " 'Partners',\n",
       " '|',\n",
       " 'Zurich',\n",
       " '(',\n",
       " 'Switzerland',\n",
       " ')',\n",
       " '|',\n",
       " '27',\n",
       " 'October',\n",
       " '2016',\n",
       " 'français',\n",
       " 'Source',\n",
       " '\\n',\n",
       " 'Current',\n",
       " 'Concerns',\n",
       " '(',\n",
       " 'Switzerland',\n",
       " ')']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_list = []\n",
    "for token in doc:\n",
    "    lemmas_list.append( token.lemma_)\n",
    "lemmas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73746d70-9ae3-4d59-8a56-689a5e8c5da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['current',\n",
       " 'concerns',\n",
       " 'n',\n",
       " '°',\n",
       " '23',\n",
       " 'october',\n",
       " '22th',\n",
       " '2016',\n",
       " 'can',\n",
       " 'the',\n",
       " 'great',\n",
       " 'nuclear',\n",
       " 'war',\n",
       " 'be',\n",
       " 'prevent',\n",
       " 'can',\n",
       " 'the',\n",
       " 'great',\n",
       " 'war',\n",
       " 'be',\n",
       " 'prevent',\n",
       " 'russia',\n",
       " 'and',\n",
       " 'china',\n",
       " 'be',\n",
       " 'prepare',\n",
       " 'for',\n",
       " 'war',\n",
       " 'right',\n",
       " 'in',\n",
       " 'front',\n",
       " 'of',\n",
       " 'america',\n",
       " '’s',\n",
       " 'doorstep',\n",
       " 'by',\n",
       " 'niki',\n",
       " 'vogt',\n",
       " 'alert',\n",
       " 'memorandum',\n",
       " 'for',\n",
       " 'obama',\n",
       " 'warn',\n",
       " 'to',\n",
       " 'defuse',\n",
       " 'tension',\n",
       " 'with',\n",
       " 'russia',\n",
       " 'by',\n",
       " 'veteran',\n",
       " 'intelligence',\n",
       " 'professionals',\n",
       " 'for',\n",
       " 'sanity',\n",
       " 'vips',\n",
       " 'us',\n",
       " 'mayors',\n",
       " 'warn',\n",
       " 'against',\n",
       " 'increase',\n",
       " 'danger',\n",
       " 'of',\n",
       " 'war',\n",
       " 'we',\n",
       " 'be',\n",
       " 'beat',\n",
       " 'to',\n",
       " 'war',\n",
       " 'interview',\n",
       " 'with',\n",
       " 'willy',\n",
       " 'wimmer',\n",
       " 'let',\n",
       " 'we',\n",
       " 'say',\n",
       " 'with',\n",
       " 'conviction',\n",
       " 'no',\n",
       " 'to',\n",
       " 'war',\n",
       " 'popular',\n",
       " 'initiative',\n",
       " 'for',\n",
       " 'nuclear',\n",
       " 'phase',\n",
       " 'out',\n",
       " 'in',\n",
       " 'switzerland',\n",
       " 'by',\n",
       " 'ernst',\n",
       " 'pauli',\n",
       " 'a',\n",
       " 'nuclear',\n",
       " 'power',\n",
       " 'plant',\n",
       " 'in',\n",
       " 'bolivia',\n",
       " 'use',\n",
       " 'lithium',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'uranium',\n",
       " 'prima',\n",
       " 'i',\n",
       " 'nostri',\n",
       " 'ticino',\n",
       " 'population',\n",
       " 'tackle',\n",
       " 'ruling',\n",
       " 'of',\n",
       " 'immigration',\n",
       " 'themselves',\n",
       " 'by',\n",
       " 'marianne',\n",
       " 'wüthrich',\n",
       " 'defend',\n",
       " 'the',\n",
       " 'identity',\n",
       " 'of',\n",
       " 'france',\n",
       " 'mean',\n",
       " 'save',\n",
       " 'our',\n",
       " 'dairy',\n",
       " 'farmer',\n",
       " 'by',\n",
       " 'natacha',\n",
       " 'polony',\n",
       " 'the',\n",
       " 'absurdity',\n",
       " 'of',\n",
       " 'today',\n",
       " '’s',\n",
       " 'credit',\n",
       " 'system',\n",
       " 'by',\n",
       " 'myret',\n",
       " 'zaki',\n",
       " 'in',\n",
       " 'great',\n",
       " 'britain',\n",
       " 'thing',\n",
       " 'be',\n",
       " 'move',\n",
       " 'after',\n",
       " 'the',\n",
       " 'brexit',\n",
       " 'by',\n",
       " 'karl',\n",
       " 'müller',\n",
       " 'language',\n",
       " 'teaching',\n",
       " 'avoid',\n",
       " 'unnecessary',\n",
       " 'quarrel',\n",
       " 'by',\n",
       " 'pierre',\n",
       " 'gabriel',\n",
       " 'bieri',\n",
       " 'partners',\n",
       " '|',\n",
       " 'zurich',\n",
       " 'switzerland',\n",
       " '|',\n",
       " '27',\n",
       " 'october',\n",
       " '2016',\n",
       " 'français',\n",
       " 'source',\n",
       " 'current',\n",
       " 'concerns',\n",
       " 'switzerland']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_.lower() for token in doc if not token.is_punct and not token.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a70a3d54-af6d-444e-8d8d-a92b60f102a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['current',\n",
       " 'concerns',\n",
       " 'n',\n",
       " '°',\n",
       " '23',\n",
       " 'october',\n",
       " '22th',\n",
       " '2016',\n",
       " 'can',\n",
       " 'the',\n",
       " 'great',\n",
       " 'nuclear',\n",
       " 'war',\n",
       " 'be',\n",
       " 'prevent',\n",
       " 'can',\n",
       " 'the',\n",
       " 'great',\n",
       " 'war',\n",
       " 'be',\n",
       " 'prevent',\n",
       " 'russia',\n",
       " 'and',\n",
       " 'china',\n",
       " 'be',\n",
       " 'prepare',\n",
       " 'for',\n",
       " 'war',\n",
       " 'right',\n",
       " 'in',\n",
       " 'front',\n",
       " 'of',\n",
       " 'america',\n",
       " '’s',\n",
       " 'doorstep',\n",
       " 'by',\n",
       " 'niki',\n",
       " 'vogt',\n",
       " 'alert',\n",
       " 'memorandum',\n",
       " 'for',\n",
       " 'obama',\n",
       " 'warn',\n",
       " 'to',\n",
       " 'defuse',\n",
       " 'tension',\n",
       " 'with',\n",
       " 'russia',\n",
       " 'by',\n",
       " 'veteran',\n",
       " 'intelligence',\n",
       " 'professionals',\n",
       " 'for',\n",
       " 'sanity',\n",
       " 'vips',\n",
       " 'us',\n",
       " 'mayors',\n",
       " 'warn',\n",
       " 'against',\n",
       " 'increase',\n",
       " 'danger',\n",
       " 'of',\n",
       " 'war',\n",
       " 'we',\n",
       " 'be',\n",
       " 'beat',\n",
       " 'to',\n",
       " 'war',\n",
       " 'interview',\n",
       " 'with',\n",
       " 'willy',\n",
       " 'wimmer',\n",
       " 'let',\n",
       " 'we',\n",
       " 'say',\n",
       " 'with',\n",
       " 'conviction',\n",
       " 'no',\n",
       " 'to',\n",
       " 'war',\n",
       " 'popular',\n",
       " 'initiative',\n",
       " 'for',\n",
       " 'nuclear',\n",
       " 'phase',\n",
       " 'out',\n",
       " 'in',\n",
       " 'switzerland',\n",
       " 'by',\n",
       " 'ernst',\n",
       " 'pauli',\n",
       " 'a',\n",
       " 'nuclear',\n",
       " 'power',\n",
       " 'plant',\n",
       " 'in',\n",
       " 'bolivia',\n",
       " 'use',\n",
       " 'lithium',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'uranium',\n",
       " 'prima',\n",
       " 'i',\n",
       " 'nostri',\n",
       " 'ticino',\n",
       " 'population',\n",
       " 'tackle',\n",
       " 'ruling',\n",
       " 'of',\n",
       " 'immigration',\n",
       " 'themselves',\n",
       " 'by',\n",
       " 'marianne',\n",
       " 'wüthrich',\n",
       " 'defend',\n",
       " 'the',\n",
       " 'identity',\n",
       " 'of',\n",
       " 'france',\n",
       " 'mean',\n",
       " 'save',\n",
       " 'our',\n",
       " 'dairy',\n",
       " 'farmer',\n",
       " 'by',\n",
       " 'natacha',\n",
       " 'polony',\n",
       " 'the',\n",
       " 'absurdity',\n",
       " 'of',\n",
       " 'today',\n",
       " '’s',\n",
       " 'credit',\n",
       " 'system',\n",
       " 'by',\n",
       " 'myret',\n",
       " 'zaki',\n",
       " 'in',\n",
       " 'great',\n",
       " 'britain',\n",
       " 'thing',\n",
       " 'be',\n",
       " 'move',\n",
       " 'after',\n",
       " 'the',\n",
       " 'brexit',\n",
       " 'by',\n",
       " 'karl',\n",
       " 'müller',\n",
       " 'language',\n",
       " 'teaching',\n",
       " 'avoid',\n",
       " 'unnecessary',\n",
       " 'quarrel',\n",
       " 'by',\n",
       " 'pierre',\n",
       " 'gabriel',\n",
       " 'bieri',\n",
       " 'partners',\n",
       " '|',\n",
       " 'zurich',\n",
       " 'switzerland',\n",
       " '|',\n",
       " '27',\n",
       " 'october',\n",
       " '2016',\n",
       " 'français',\n",
       " 'source',\n",
       " 'current',\n",
       " 'concerns',\n",
       " 'switzerland']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_list = []\n",
    "for token in doc:\n",
    "    if token.is_punct:\n",
    "        continue\n",
    "    if token.is_space:\n",
    "        continue\n",
    "\n",
    "    lemmas_list.append(token.lemma_.lower())\n",
    "\n",
    "lemmas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b30eee92-4a4f-4b06-b42c-3e7ce7a3e8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['current',\n",
       " 'concerns',\n",
       " 'n',\n",
       " '°',\n",
       " '23',\n",
       " 'october',\n",
       " '22th',\n",
       " '2016',\n",
       " 'great',\n",
       " 'nuclear',\n",
       " 'war',\n",
       " 'prevent',\n",
       " 'great',\n",
       " 'war',\n",
       " 'prevent',\n",
       " 'russia',\n",
       " 'china',\n",
       " 'prepare',\n",
       " 'war',\n",
       " 'right',\n",
       " 'america',\n",
       " 'doorstep',\n",
       " 'niki',\n",
       " 'vogt',\n",
       " 'alert',\n",
       " 'memorandum',\n",
       " 'obama',\n",
       " 'warn',\n",
       " 'defuse',\n",
       " 'tension',\n",
       " 'russia',\n",
       " 'veteran',\n",
       " 'intelligence',\n",
       " 'professionals',\n",
       " 'sanity',\n",
       " 'vips',\n",
       " 'mayors',\n",
       " 'warn',\n",
       " 'increase',\n",
       " 'danger',\n",
       " 'war',\n",
       " 'beat',\n",
       " 'war',\n",
       " 'interview',\n",
       " 'willy',\n",
       " 'wimmer',\n",
       " 'let',\n",
       " 'conviction',\n",
       " 'war',\n",
       " 'popular',\n",
       " 'initiative',\n",
       " 'nuclear',\n",
       " 'phase',\n",
       " 'switzerland',\n",
       " 'ernst',\n",
       " 'pauli',\n",
       " 'nuclear',\n",
       " 'power',\n",
       " 'plant',\n",
       " 'bolivia',\n",
       " 'lithium',\n",
       " 'instead',\n",
       " 'uranium',\n",
       " 'prima',\n",
       " 'nostri',\n",
       " 'ticino',\n",
       " 'population',\n",
       " 'tackle',\n",
       " 'ruling',\n",
       " 'immigration',\n",
       " 'marianne',\n",
       " 'wüthrich',\n",
       " 'defend',\n",
       " 'identity',\n",
       " 'france',\n",
       " 'mean',\n",
       " 'save',\n",
       " 'dairy',\n",
       " 'farmer',\n",
       " 'natacha',\n",
       " 'polony',\n",
       " 'absurdity',\n",
       " 'today',\n",
       " 'credit',\n",
       " 'system',\n",
       " 'myret',\n",
       " 'zaki',\n",
       " 'great',\n",
       " 'britain',\n",
       " 'thing',\n",
       " 'move',\n",
       " 'brexit',\n",
       " 'karl',\n",
       " 'müller',\n",
       " 'language',\n",
       " 'teaching',\n",
       " 'avoid',\n",
       " 'unnecessary',\n",
       " 'quarrel',\n",
       " 'pierre',\n",
       " 'gabriel',\n",
       " 'bieri',\n",
       " 'partners',\n",
       " '|',\n",
       " 'zurich',\n",
       " 'switzerland',\n",
       " '|',\n",
       " '27',\n",
       " 'october',\n",
       " '2016',\n",
       " 'français',\n",
       " 'source',\n",
       " 'current',\n",
       " 'concerns',\n",
       " 'switzerland']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_.lower() for token in doc if not token.is_punct and not token.is_space and not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bacd3aac-c75b-4700-81ab-5813004e8622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['current',\n",
       " 'concerns',\n",
       " 'n',\n",
       " '°',\n",
       " '23',\n",
       " 'october',\n",
       " '22th',\n",
       " '2016',\n",
       " 'great',\n",
       " 'nuclear',\n",
       " 'war',\n",
       " 'prevent',\n",
       " 'great',\n",
       " 'war',\n",
       " 'prevent',\n",
       " 'russia',\n",
       " 'china',\n",
       " 'prepare',\n",
       " 'war',\n",
       " 'right',\n",
       " 'america',\n",
       " 'doorstep',\n",
       " 'niki',\n",
       " 'vogt',\n",
       " 'alert',\n",
       " 'memorandum',\n",
       " 'obama',\n",
       " 'warn',\n",
       " 'defuse',\n",
       " 'tension',\n",
       " 'russia',\n",
       " 'veteran',\n",
       " 'intelligence',\n",
       " 'professionals',\n",
       " 'sanity',\n",
       " 'vips',\n",
       " 'mayors',\n",
       " 'warn',\n",
       " 'increase',\n",
       " 'danger',\n",
       " 'war',\n",
       " 'beat',\n",
       " 'war',\n",
       " 'interview',\n",
       " 'willy',\n",
       " 'wimmer',\n",
       " 'let',\n",
       " 'conviction',\n",
       " 'war',\n",
       " 'popular',\n",
       " 'initiative',\n",
       " 'nuclear',\n",
       " 'phase',\n",
       " 'switzerland',\n",
       " 'ernst',\n",
       " 'pauli',\n",
       " 'nuclear',\n",
       " 'power',\n",
       " 'plant',\n",
       " 'bolivia',\n",
       " 'lithium',\n",
       " 'instead',\n",
       " 'uranium',\n",
       " 'prima',\n",
       " 'nostri',\n",
       " 'ticino',\n",
       " 'population',\n",
       " 'tackle',\n",
       " 'ruling',\n",
       " 'immigration',\n",
       " 'marianne',\n",
       " 'wüthrich',\n",
       " 'defend',\n",
       " 'identity',\n",
       " 'france',\n",
       " 'mean',\n",
       " 'save',\n",
       " 'dairy',\n",
       " 'farmer',\n",
       " 'natacha',\n",
       " 'polony',\n",
       " 'absurdity',\n",
       " 'today',\n",
       " 'credit',\n",
       " 'system',\n",
       " 'myret',\n",
       " 'zaki',\n",
       " 'great',\n",
       " 'britain',\n",
       " 'thing',\n",
       " 'move',\n",
       " 'brexit',\n",
       " 'karl',\n",
       " 'müller',\n",
       " 'language',\n",
       " 'teaching',\n",
       " 'avoid',\n",
       " 'unnecessary',\n",
       " 'quarrel',\n",
       " 'pierre',\n",
       " 'gabriel',\n",
       " 'bieri',\n",
       " 'partners',\n",
       " '|',\n",
       " 'zurich',\n",
       " 'switzerland',\n",
       " '|',\n",
       " '27',\n",
       " 'october',\n",
       " '2016',\n",
       " 'français',\n",
       " 'source',\n",
       " 'current',\n",
       " 'concerns',\n",
       " 'switzerland']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## [v3 For Loop - Continue] Make a list of only the tokens in the document that are not punctuation or spaces \n",
    "## Lower the casing as well\n",
    "lemmas_list = []\n",
    "for token in doc:\n",
    "    if token.is_punct:\n",
    "        continue\n",
    "    if token.is_space:\n",
    "        continue\n",
    "    if token.is_stop:\n",
    "        continue\n",
    "\n",
    "    lemmas_list.append(token.lemma_.lower())\n",
    "\n",
    "lemmas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a923a8d-3964-4265-abf5-8f8123142d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>lower_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>spacy_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A whirlwind day in D.C. showcases Trump’s unor...</td>\n",
       "      <td>Donald Trump endorsed an unabashedly noninterv...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>donald trump endorsed an unabashedly noninterv...</td>\n",
       "      <td>[donald, trump, endorsed, an, unabashedly, non...</td>\n",
       "      <td>[donald, trump, endorse, unabashedly, noninter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Baltimore's call for federal police probe, ...</td>\n",
       "      <td>While some Justice Department investigations a...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>while some justice department investigations a...</td>\n",
       "      <td>[while, some, justice, department, investigati...</td>\n",
       "      <td>[justice, department, investigation, adversari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump Proudly Declares: Most Of The People I’v...</td>\n",
       "      <td>Trump Proudly Declares: Most Of The People I’v...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>trump proudly declares: most of the people i’v...</td>\n",
       "      <td>[trump, proudly, declares:, most, of, the, peo...</td>\n",
       "      <td>[trump, proudly, declare, people, insult, dese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inside the Trump-Bush melodrama: Decades of te...</td>\n",
       "      <td>Donald Trump spent a day in January 2014 hobno...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>donald trump spent a day in january 2014 hobno...</td>\n",
       "      <td>[donald, trump, spent, a, day, in, january, 20...</td>\n",
       "      <td>[donald, trump, spend, day, january, 2014, hob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shutdown clash to return in force by December</td>\n",
       "      <td>Notable names include Ray Washburne (Commerce)...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>notable names include ray washburne (commerce)...</td>\n",
       "      <td>[notable, names, include, ray, washburne, (com...</td>\n",
       "      <td>[notable, name, include, ray, washburne, comme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  A whirlwind day in D.C. showcases Trump’s unor...   \n",
       "1  In Baltimore's call for federal police probe, ...   \n",
       "2  Trump Proudly Declares: Most Of The People I’v...   \n",
       "3  Inside the Trump-Bush melodrama: Decades of te...   \n",
       "4      Shutdown clash to return in force by December   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Donald Trump endorsed an unabashedly noninterv...  REAL   \n",
       "1  While some Justice Department investigations a...  REAL   \n",
       "2  Trump Proudly Declares: Most Of The People I’v...  FAKE   \n",
       "3  Donald Trump spent a day in January 2014 hobno...  REAL   \n",
       "4  Notable names include Ray Washburne (Commerce)...  REAL   \n",
       "\n",
       "                                          lower_text  \\\n",
       "0  donald trump endorsed an unabashedly noninterv...   \n",
       "1  while some justice department investigations a...   \n",
       "2  trump proudly declares: most of the people i’v...   \n",
       "3  donald trump spent a day in january 2014 hobno...   \n",
       "4  notable names include ray washburne (commerce)...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [donald, trump, endorsed, an, unabashedly, non...   \n",
       "1  [while, some, justice, department, investigati...   \n",
       "2  [trump, proudly, declares:, most, of, the, peo...   \n",
       "3  [donald, trump, spent, a, day, in, january, 20...   \n",
       "4  [notable, names, include, ray, washburne, (com...   \n",
       "\n",
       "                                        spacy_lemmas  \n",
       "0  [donald, trump, endorse, unabashedly, noninter...  \n",
       "1  [justice, department, investigation, adversari...  \n",
       "2  [trump, proudly, declare, people, insult, dese...  \n",
       "3  [donald, trump, spend, day, january, 2014, hob...  \n",
       "4  [notable, name, include, ray, washburne, comme...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## [V1 List Comp] Define a function to use spacy to process our text\n",
    "def spacy_process(text):\n",
    "        \"\"\"Lemmatize tokens, lower case, remove punctuation, spaces, and stop words\"\"\"\n",
    "        doc = nlp_model(text)\n",
    "        processed_doc = [token.lemma_.lower() for token in doc if not token.is_punct and \n",
    "                         not token.is_space and not token.is_stop and \n",
    "                         not 'http' in token.lemma_.lower() and 'www' not in token.lemma_.lower()]\n",
    "        return processed_doc\n",
    "\n",
    "## process the tweets using the spacy function\n",
    "df['spacy_lemmas'] = df['text'].apply(spacy_process)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b421ad66-cc8f-4100-b4ba-6f86bd574b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REAL    0.500552\n",
       "FAKE    0.499448\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check class balance of 'rating'\n",
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9581b080-58d0-46ef-b1f4-345b859f5570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL label\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>lower_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>spacy_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A whirlwind day in D.C. showcases Trump’s unor...</td>\n",
       "      <td>Donald Trump endorsed an unabashedly noninterv...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>donald trump endorsed an unabashedly noninterv...</td>\n",
       "      <td>[donald, trump, endorsed, an, unabashedly, non...</td>\n",
       "      <td>[donald, trump, endorse, unabashedly, noninter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Baltimore's call for federal police probe, ...</td>\n",
       "      <td>While some Justice Department investigations a...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>while some justice department investigations a...</td>\n",
       "      <td>[while, some, justice, department, investigati...</td>\n",
       "      <td>[justice, department, investigation, adversari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inside the Trump-Bush melodrama: Decades of te...</td>\n",
       "      <td>Donald Trump spent a day in January 2014 hobno...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>donald trump spent a day in january 2014 hobno...</td>\n",
       "      <td>[donald, trump, spent, a, day, in, january, 20...</td>\n",
       "      <td>[donald, trump, spend, day, january, 2014, hob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shutdown clash to return in force by December</td>\n",
       "      <td>Notable names include Ray Washburne (Commerce)...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>notable names include ray washburne (commerce)...</td>\n",
       "      <td>[notable, names, include, ray, washburne, (com...</td>\n",
       "      <td>[notable, name, include, ray, washburne, comme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Democratic debate 2015: Hillary Clinton and Be...</td>\n",
       "      <td>Watch the first Democratic presidential debate...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>watch the first democratic presidential debate...</td>\n",
       "      <td>[watch, the, first, democratic, presidential, ...</td>\n",
       "      <td>[watch, democratic, presidential, debate, tues...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  A whirlwind day in D.C. showcases Trump’s unor...   \n",
       "1  In Baltimore's call for federal police probe, ...   \n",
       "3  Inside the Trump-Bush melodrama: Decades of te...   \n",
       "4      Shutdown clash to return in force by December   \n",
       "7  Democratic debate 2015: Hillary Clinton and Be...   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Donald Trump endorsed an unabashedly noninterv...  REAL   \n",
       "1  While some Justice Department investigations a...  REAL   \n",
       "3  Donald Trump spent a day in January 2014 hobno...  REAL   \n",
       "4  Notable names include Ray Washburne (Commerce)...  REAL   \n",
       "7  Watch the first Democratic presidential debate...  REAL   \n",
       "\n",
       "                                          lower_text  \\\n",
       "0  donald trump endorsed an unabashedly noninterv...   \n",
       "1  while some justice department investigations a...   \n",
       "3  donald trump spent a day in january 2014 hobno...   \n",
       "4  notable names include ray washburne (commerce)...   \n",
       "7  watch the first democratic presidential debate...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [donald, trump, endorsed, an, unabashedly, non...   \n",
       "1  [while, some, justice, department, investigati...   \n",
       "3  [donald, trump, spent, a, day, in, january, 20...   \n",
       "4  [notable, names, include, ray, washburne, (com...   \n",
       "7  [watch, the, first, democratic, presidential, ...   \n",
       "\n",
       "                                        spacy_lemmas  \n",
       "0  [donald, trump, endorse, unabashedly, noninter...  \n",
       "1  [justice, department, investigation, adversari...  \n",
       "3  [donald, trump, spend, day, january, 2014, hob...  \n",
       "4  [notable, name, include, ray, washburne, comme...  \n",
       "7  [watch, democratic, presidential, debate, tues...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE label\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>lower_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>spacy_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump Proudly Declares: Most Of The People I’v...</td>\n",
       "      <td>Trump Proudly Declares: Most Of The People I’v...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>trump proudly declares: most of the people i’v...</td>\n",
       "      <td>[trump, proudly, declares:, most, of, the, peo...</td>\n",
       "      <td>[trump, proudly, declare, people, insult, dese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can the great nuclear war be prevented ?</td>\n",
       "      <td>«Current Concerns», n°23, October 22th, 2016\\n...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>«current concerns», n°23, october 22th, 2016\\n...</td>\n",
       "      <td>[«current, concerns»,, n°23,, october, 22th,, ...</td>\n",
       "      <td>[current, concerns, n, °, 23, october, 22th, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>US charges 61 with India-based scam involving ...</td>\n",
       "      <td>US charges 61 with India-based scam involving ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>us charges 61 with india-based scam involving ...</td>\n",
       "      <td>[us, charges, 61, with, india-based, scam, inv...</td>\n",
       "      <td>[charge, 61, india, base, scam, involve, 15,00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What DNC Donors REALLY think of African Americans</td>\n",
       "      <td>This Video is REALLY Disturbing... \\nNot just ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>this video is really disturbing... \\nnot just ...</td>\n",
       "      <td>[this, video, is, really, disturbing..., not, ...</td>\n",
       "      <td>[video, disturbing, african, americans, americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Must Read of the Day – Dennis Kucinich’s Extra...</td>\n",
       "      <td>at 1:10 pm 3 Comments \\nWAR is a racket. It al...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>at 1:10 pm 3 comments \\nwar is a racket. it al...</td>\n",
       "      <td>[at, 1:10, pm, 3, comments, war, is, a, racket...</td>\n",
       "      <td>[1:10, pm, 3, comment, war, racket, possibly, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "2   Trump Proudly Declares: Most Of The People I’v...   \n",
       "5            Can the great nuclear war be prevented ?   \n",
       "6   US charges 61 with India-based scam involving ...   \n",
       "8   What DNC Donors REALLY think of African Americans   \n",
       "11  Must Read of the Day – Dennis Kucinich’s Extra...   \n",
       "\n",
       "                                                 text label  \\\n",
       "2   Trump Proudly Declares: Most Of The People I’v...  FAKE   \n",
       "5   «Current Concerns», n°23, October 22th, 2016\\n...  FAKE   \n",
       "6   US charges 61 with India-based scam involving ...  FAKE   \n",
       "8   This Video is REALLY Disturbing... \\nNot just ...  FAKE   \n",
       "11  at 1:10 pm 3 Comments \\nWAR is a racket. It al...  FAKE   \n",
       "\n",
       "                                           lower_text  \\\n",
       "2   trump proudly declares: most of the people i’v...   \n",
       "5   «current concerns», n°23, october 22th, 2016\\n...   \n",
       "6   us charges 61 with india-based scam involving ...   \n",
       "8   this video is really disturbing... \\nnot just ...   \n",
       "11  at 1:10 pm 3 comments \\nwar is a racket. it al...   \n",
       "\n",
       "                                               tokens  \\\n",
       "2   [trump, proudly, declares:, most, of, the, peo...   \n",
       "5   [«current, concerns»,, n°23,, october, 22th,, ...   \n",
       "6   [us, charges, 61, with, india-based, scam, inv...   \n",
       "8   [this, video, is, really, disturbing..., not, ...   \n",
       "11  [at, 1:10, pm, 3, comments, war, is, a, racket...   \n",
       "\n",
       "                                         spacy_lemmas  \n",
       "2   [trump, proudly, declare, people, insult, dese...  \n",
       "5   [current, concerns, n, °, 23, october, 22th, 2...  \n",
       "6   [charge, 61, india, base, scam, involve, 15,00...  \n",
       "8   [video, disturbing, african, americans, americ...  \n",
       "11  [1:10, pm, 3, comment, war, racket, possibly, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Divide documents by sentiment\n",
    "high = df.loc[df['label'] == 'REAL']\n",
    "low = df.loc[df['label'] == 'FAKE']\n",
    "print('REAL label')\n",
    "display(high.head())\n",
    "print('FAKE label')\n",
    "display(low.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "944788b9-ad60-440e-ae3a-bce21a611f41",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "could not convert string to float: 'REAL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\core\\nanops.py:760\u001b[0m, in \u001b[0;36mnanmedian\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 760\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;66;03m# e.g. \"could not convert string to float: 'a'\"\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'REAL'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## visualize median review length\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m high_len \u001b[38;5;241m=\u001b[39m \u001b[43mhigh\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m low_len \u001b[38;5;241m=\u001b[39m low[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmedian()\n\u001b[0;32m      5\u001b[0m ax \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mbarplot(data\u001b[38;5;241m=\u001b[39mdf, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m'\u001b[39m, estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m,);\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\core\\generic.py:11917\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.median\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11899\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m  11900\u001b[0m     _num_doc,\n\u001b[0;32m  11901\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn the median of the values over the requested axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11915\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11916\u001b[0m ):\n\u001b[1;32m> 11917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mmedian(\u001b[38;5;28mself\u001b[39m, axis, skipna, level, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\core\\generic.py:11413\u001b[0m, in \u001b[0;36mNDFrame.median\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmedian\u001b[39m(\n\u001b[0;32m  11406\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  11407\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m lib\u001b[38;5;241m.\u001b[39mNoDefault \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mno_default,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11411\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11412\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m> 11413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[0;32m  11414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmedian, axis, skipna, level, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  11415\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\core\\generic.py:11353\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11343\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m  11344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the level keyword in DataFrame and Series aggregations is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m  11345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be removed in a future version. Use groupby \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11348\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m  11349\u001b[0m     )\n\u001b[0;32m  11350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_by_level(\n\u001b[0;32m  11351\u001b[0m         name, axis\u001b[38;5;241m=\u001b[39maxis, level\u001b[38;5;241m=\u001b[39mlevel, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[0;32m  11352\u001b[0m     )\n\u001b[1;32m> 11353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  11354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[0;32m  11355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\core\\series.py:4816\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   4812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   4813\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not implement \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4814\u001b[0m     )\n\u001b[0;32m   4815\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 4816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(delegate, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\core\\nanops.py:155\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    153\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\pandas\\core\\nanops.py:763\u001b[0m, in \u001b[0;36mnanmedian\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    760\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;66;03m# e.g. \"could not convert string to float: 'a'\"\u001b[39;00m\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(err)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    765\u001b[0m     values[mask] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n",
      "\u001b[1;31mTypeError\u001b[0m: could not convert string to float: 'REAL'"
     ]
    }
   ],
   "source": [
    "## visualize median review length\n",
    "high_len = high['label'].median()\n",
    "low_len = low['label'].median()\n",
    "\n",
    "ax = sns.barplot(data=df, x='rating', y='length', estimator='median',);\n",
    "\n",
    "# Show plot before print statement\n",
    "plt.show()\n",
    "print(f' The median character length for {low_len} for low Ratings and {high_len} for high ratings.')\n",
    "\n",
    "# Save figure\n",
    "fig = ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d237ad-eb96-4011-bc0c-f6d286dc73ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
